{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132152ae-902c-4506-8edc-b5b6b8c5b549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Running Ablation 0 on Model Salesforce/blip2-opt-2.7b\n"
     ]
    }
   ],
   "source": [
    "ABL_NUM = 0\n",
    "ABL = \"abl\"+str(ABL_NUM)\n",
    "model_name = \"Salesforce/blip2-opt-2.7b\"\n",
    "model_save_path = f\"Finetuned_{ABL}\"\n",
    "print(f\"Currently Running Ablation {ABL_NUM} on Model {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85f133e-6d16-4002-bb6f-ad2ae1b78810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmistha/miniconda3/envs/vllm2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoProcessor, AutoModelForVisualQuestionAnswering\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3759b7ce-604f-4d42-8650-43b0dbb1c086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651268ffc5fe4e87897da935aadfb099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x15551aa0b6b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForVisualQuestionAnswering.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "torch.cuda.empty_cache()\n",
    "torch.manual_seed(42)\n",
    "# config = LoraConfig(\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"k_proj\"]\n",
    "# )\n",
    "# model = get_peft_model(model, config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4109ef21-41d5-4b6e-ae85-a3a1d05bb1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor, ablation):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.ablation = ablation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        emotion = self.dataset[idx]['emotion']\n",
    "        sentiment = self.dataset[idx]['sentiment']\n",
    "        sarcasm = self.dataset[idx]['sarcasm']\n",
    "        if self.ablation == 'abl0':\n",
    "            question = f\"What complaint is conveyed by the disappointed and frustrated user in the video?\"\n",
    "        elif self.ablation == 'abl1':  # Emotion only\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed?\"\n",
    "        elif self.ablation == 'abl2':  # Sentiment only\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {sentiment} sentiment?\"\n",
    "        elif self.ablation == 'abl3':  # Sarcasm only\n",
    "            question = f\"What complaint is conveyed by the user in the video, noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl4':  # Emotion + Sentiment\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and the {sentiment} sentiment?\"\n",
    "        elif self.ablation == 'abl5':  # Sentiment + Sarcasm\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {sentiment} sentiment and noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl6':  # Emotion + Sarcasm\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl7':  # All\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and the {sentiment} sentiment. Additionally, note if the complaint is being conveyed in a {sarcasm} sarcastic manner.\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid ablation type. Choose from abl1 to abl7.\")\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        image = self.dataset[idx]['image']\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = image.convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        encoding = self.processor( image, question, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        labels = self.processor.tokenizer.encode( answer, max_length=256, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "        encoding[\"labels\"] = labels\n",
    "        for k, v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        return encoding\n",
    "dataset = load_dataset(\"cerelac2/consumer-complaint-vqa\")\n",
    "train_dataset = dataset['train']\n",
    "split_datasets = train_dataset.train_test_split(test_size=0.05)\n",
    "train_split = split_datasets['train']\n",
    "valid_split = split_datasets['test']\n",
    "train_dataset = VQADataset(dataset=train_split, processor=processor, ablation=ABL)\n",
    "valid_dataset = VQADataset(dataset=valid_split, processor=processor, ablation=ABL)\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "446e9ebb-facc-468e-b294-cebe02bae474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 1: 100%|██████████| 161/161 [01:30<00:00,  1.78it/s]\n",
      "Validating epoch 1: 100%|██████████| 9/9 [00:03<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training loss: 7.676483302382949 - Eval Loss: 7.620656119452582 - LR: 4e-05\n",
      "Saved model to Finetuned_abl0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 2: 100%|██████████| 161/161 [01:29<00:00,  1.79it/s]\n",
      "Validating epoch 2: 100%|██████████| 9/9 [00:02<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training loss: 7.673102171524711 - Eval Loss: 7.620656119452582 - LR: 3.6e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 3: 100%|██████████| 161/161 [01:30<00:00,  1.78it/s]\n",
      "Validating epoch 3: 100%|██████████| 9/9 [00:02<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training loss: 7.660724995299156 - Eval Loss: 7.620656119452582 - LR: 3.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch 4: 100%|██████████| 161/161 [01:30<00:00,  1.78it/s]\n",
      "Validating epoch 4: 100%|██████████| 9/9 [00:02<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training loss: 7.6649066232006 - Eval Loss: 7.620656119452582 - LR: 2.9160000000000002e-05\n",
      "Finetuning complete!\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
    "num_epochs = 4\n",
    "patience = 10\n",
    "min_eval_loss = float(\"inf\")\n",
    "early_stopping_hook = 0\n",
    "tracking_information = []\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Training epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_masked = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "        scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    for batch in tqdm(valid_dataloader, desc=f\"Validating epoch {epoch+1}\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        attention_masked = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        with torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            eval_loss += loss.item()\n",
    "    tracking_information.append((epoch_loss / len(train_dataloader), eval_loss / len(valid_dataloader), optimizer.param_groups[0][\"lr\"]))\n",
    "    print(f\"Epoch {epoch+1} - Training loss: {epoch_loss / len(train_dataloader)} - Eval Loss: {eval_loss / len(valid_dataloader)} - LR: {optimizer.param_groups[0]['lr']}\")\n",
    "    scheduler.step()\n",
    "    if eval_loss < min_eval_loss:\n",
    "        model.save_pretrained(model_save_path)\n",
    "        print(f\"Saved model to {model_save_path}\")\n",
    "        min_eval_loss = eval_loss\n",
    "        early_stopping_hook = 0\n",
    "    else:\n",
    "        early_stopping_hook += 1\n",
    "        if early_stopping_hook > patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pickle.dump(tracking_information, open(f\"tracking_information_{ABL}.pkl\", \"wb\"))\n",
    "print(\"Finetuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d440454-01ed-4999-9951-c70d82ae4c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2b2fa68c934790af0fd4c837909b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 0, but `max_length` is set to -78. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m encodings, answers, ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     71\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(finetuned_model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m encodings\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfinetuned_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m51\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(outputs):\n\u001b[1;32m     74\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/models/blip_2/modeling_blip_2.py:1835\u001b[0m, in \u001b[0;36mBlip2ForConditionalGeneration.generate\u001b[0;34m(self, pixel_values, input_ids, attention_mask, **generate_kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m20\u001b[39m) \u001b[38;5;241m+\u001b[39m language_model_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1833\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m language_model_inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1835\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlanguage_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1836\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1838\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[38;5;66;03m# this is a temporary workaround to be consistent with other generation models and\u001b[39;00m\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;66;03m# have BOS as the first token, even though under the hood we are calling LM with embeds\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/generation/utils.py:1499\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1494\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1496\u001b[0m             )\n\u001b[1;32m   1497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1499\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/generation/utils.py:1149\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1148\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1150\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1151\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1153\u001b[0m     )\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1157\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1159\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 0, but `max_length` is set to -78. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BlipForQuestionAnswering\n",
    "dataset = load_dataset(\"cerelac2/consumer-complaint-vqa\")\n",
    "\n",
    "class VQATestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, processor, ablation):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.ablation = ablation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.dataset[idx]['ID']\n",
    "        emotion = self.dataset[idx]['emotion']\n",
    "        sentiment = self.dataset[idx]['sentiment']\n",
    "        sarcasm = self.dataset[idx]['sarcasm']\n",
    "        if self.ablation == 'abl0':\n",
    "            question = f\"What complaint is conveyed by the disappointed and frustrated user in the video?\"\n",
    "        elif self.ablation == 'abl1':  # Emotion only\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed?\"\n",
    "        elif self.ablation == 'abl2':  # Sentiment only\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {sentiment} sentiment?\"\n",
    "        elif self.ablation == 'abl3':  # Sarcasm only\n",
    "            question = f\"What complaint is conveyed by the user in the video, noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl4':  # Emotion + Sentiment\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and the {sentiment} sentiment?\"\n",
    "        elif self.ablation == 'abl5':  # Sentiment + Sarcasm\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {sentiment} sentiment and noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl6':  # Emotion + Sarcasm\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and noting if it is conveyed in a {sarcasm} sarcastic manner?\"\n",
    "        elif self.ablation == 'abl7':  # All\n",
    "            question = f\"What complaint is conveyed by the user in the video, considering the {emotion} expressed and the {sentiment} sentiment. Additionally, note if the complaint is being conveyed in a {sarcasm} sarcastic manner.\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid ablation type. Choose from abl1 to abl7.\")\n",
    "\n",
    "        answer = self.dataset[idx]['answer']\n",
    "        image = self.dataset[idx]['image']\n",
    "        \n",
    "        # Load the image if it's a path\n",
    "        if isinstance(image, Image.Image):\n",
    "            image = image.convert(\"RGB\")\n",
    "        else:\n",
    "            image = Image.open(image).convert(\"RGB\")\n",
    "        \n",
    "        encoding = self.processor(images=image, text=question, return_tensors=\"pt\", padding=\"max_length\", truncation=True,max_length=128)\n",
    "        \n",
    "        return encoding, answer, video_id\n",
    "\n",
    "# Initialize dataset, model, and evaluation\n",
    "test_dataset = VQATestDataset(dataset=dataset['test'], processor=processor, ablation=ABL)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # Set batch size according to your needs\n",
    "finetuned_model =  AutoModelForVisualQuestionAnswering.from_pretrained(model_save_path)\n",
    "finetuned_model.to(\"cuda\")\n",
    "\n",
    "predicted_answers = []\n",
    "ground_truths = []\n",
    "video_ids = []\n",
    "\n",
    "print(\"Evaluating on Test Set:\")\n",
    "finetuned_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        encodings, answers, ids = batch\n",
    "        inputs = {k: v.squeeze(1).to(finetuned_model.device) for k, v in encodings.items()}\n",
    "        outputs = finetuned_model.generate(**inputs,max_length=51)\n",
    "        for i, output in enumerate(outputs):\n",
    "            predicted_answer = processor.decode(output, skip_special_tokens=True)\n",
    "            predicted_answers.append(predicted_answer)\n",
    "            ground_truths.append(answers[i])\n",
    "            video_ids.append(ids[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629a1a45-e918-4b96-bf41-b3823a52c9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to blip2_abl0_all_results.csv\n"
     ]
    }
   ],
   "source": [
    "csv_filename = f\"blip2_{ABL}_all_results.csv\"\n",
    "\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"video_id\", \"predicted_answer\", \"ground_truth\"])\n",
    "    \n",
    "    for video_id, pred, gt in zip(video_ids, predicted_answers, ground_truths):\n",
    "        writer.writerow([video_id, pred, gt])\n",
    "\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55627ac7-f6f1-43a5-95e3-273221af8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/sarmistha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/sarmistha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/sarmistha/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/sarmistha/miniconda3/envs/vllm2/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m flesch_scores \u001b[38;5;241m=\u001b[39m [textstat\u001b[38;5;241m.\u001b[39mflesch_reading_ease(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[1;32m     56\u001b[0m coleman_liau_scores \u001b[38;5;241m=\u001b[39m [textstat\u001b[38;5;241m.\u001b[39mcoleman_liau_index(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[0;32m---> 57\u001b[0m perplexity_scores \u001b[38;5;241m=\u001b[39m [calculate_perplexity(pred) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[1;32m     58\u001b[0m meteor_scores \u001b[38;5;241m=\u001b[39m [meteor_score([ref\u001b[38;5;241m.\u001b[39msplit()], pred\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predicted_answers, ground_truths)]\n\u001b[1;32m     59\u001b[0m hamming_distances \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(el1 \u001b[38;5;241m!=\u001b[39m el2 \u001b[38;5;28;01mfor\u001b[39;00m el1, el2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, ref)) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predicted_answers, ground_truths)]\n",
      "Cell \u001b[0;32mIn[7], line 57\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m flesch_scores \u001b[38;5;241m=\u001b[39m [textstat\u001b[38;5;241m.\u001b[39mflesch_reading_ease(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[1;32m     56\u001b[0m coleman_liau_scores \u001b[38;5;241m=\u001b[39m [textstat\u001b[38;5;241m.\u001b[39mcoleman_liau_index(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[0;32m---> 57\u001b[0m perplexity_scores \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_answers]\n\u001b[1;32m     58\u001b[0m meteor_scores \u001b[38;5;241m=\u001b[39m [meteor_score([ref\u001b[38;5;241m.\u001b[39msplit()], pred\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predicted_answers, ground_truths)]\n\u001b[1;32m     59\u001b[0m hamming_distances \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(el1 \u001b[38;5;241m!=\u001b[39m el2 \u001b[38;5;28;01mfor\u001b[39;00m el1, el2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred, ref)) \u001b[38;5;28;01mfor\u001b[39;00m pred, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predicted_answers, ground_truths)]\n",
      "Cell \u001b[0;32mIn[7], line 36\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     34\u001b[0m inputs \u001b[38;5;241m=\u001b[39m gpt2_tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     log_likelihood \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mexp(torch\u001b[38;5;241m.\u001b[39mtensor(log_likelihood))\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1305\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1305\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1320\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1119\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1108\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1109\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         output_attentions,\n\u001b[1;32m   1117\u001b[0m     )\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:654\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    652\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    653\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> 654\u001b[0m feed_forward_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[1;32m    656\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:576\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    574\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(hidden_states)\n\u001b[1;32m    575\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(hidden_states)\n\u001b[0;32m--> 576\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/vllm2/lib/python3.10/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from textstat import textstat\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from moverscore import get_idf_dict, word_mover_score\n",
    "import torch\n",
    "import numpy as np\n",
    "input_csv = f\"blip2_{ABL}_all_results.csv\"\n",
    "data = pd.read_csv(input_csv)\n",
    "predicted_answers = data[\"predicted_answer\"].tolist()\n",
    "ground_truths = data[\"ground_truth\"].tolist()\n",
    "video_ids = data[\"video_id\"].tolist()\n",
    "\n",
    "# Load the models and metrics\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bert_score_metric = evaluate.load(\"bertscore\")\n",
    "meteor_metric = evaluate.load(\"meteor\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define helper functions\n",
    "def calculate_bleu(pred, ref):\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    return [\n",
    "        sentence_bleu([ref], pred, weights=(1, 0, 0, 0), smoothing_function=smoothing),\n",
    "        sentence_bleu([ref], pred, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing),\n",
    "        sentence_bleu([ref], pred, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "    ]\n",
    "\n",
    "def calculate_perplexity(text):\n",
    "    inputs = gpt2_tokenizer(text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "        log_likelihood = outputs.loss.item()\n",
    "    return torch.exp(torch.tensor(log_likelihood))\n",
    "\n",
    "def calculate_jaccard(pred, ref):\n",
    "    pred_set = set(pred.split())\n",
    "    ref_set = set(ref.split())\n",
    "    intersection = len(pred_set.intersection(ref_set))\n",
    "    union = len(pred_set.union(ref_set))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Calculate individual metrics\n",
    "idf_dict_hyp = get_idf_dict(predicted_answers)\n",
    "idf_dict_ref = get_idf_dict(ground_truths)\n",
    "mover_scores = word_mover_score(ground_truths, predicted_answers, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=True)\n",
    "\n",
    "rouge_scores = rouge_metric.compute(predictions=predicted_answers, references=ground_truths)\n",
    "bleu_scores = [calculate_bleu(pred, ref) for pred, ref in zip(predicted_answers, ground_truths)]\n",
    "bert_scores = bert_score_metric.compute(predictions=predicted_answers, references=ground_truths, lang=\"en\")\n",
    "flesch_scores = [textstat.flesch_reading_ease(text) for text in predicted_answers]\n",
    "coleman_liau_scores = [textstat.coleman_liau_index(text) for text in predicted_answers]\n",
    "perplexity_scores = [calculate_perplexity(pred) for pred in predicted_answers]\n",
    "meteor_scores = [meteor_score([ref.split()], pred.split()) for pred, ref in zip(predicted_answers, ground_truths)]\n",
    "hamming_distances = [sum(el1 != el2 for el1, el2 in zip(pred, ref)) for pred, ref in zip(predicted_answers, ground_truths)]\n",
    "jaccard_similarities = [calculate_jaccard(pred, ref) for pred, ref in zip(predicted_answers, ground_truths)]\n",
    "\n",
    "metrics_summary = {\n",
    "    \"ROUGE-1\": np.mean(rouge_scores[\"rouge1\"]),\n",
    "    \"ROUGE-2\": np.mean(rouge_scores[\"rouge2\"]),\n",
    "    \"ROUGE-L\": np.mean(rouge_scores[\"rougeL\"]),\n",
    "    \"BLEU-1\": np.mean([score[0] for score in bleu_scores]),\n",
    "    \"BLEU-2\": np.mean([score[1] for score in bleu_scores]),\n",
    "    \"BLEU-3\": np.mean([score[2] for score in bleu_scores]),\n",
    "    \"BERT Score\": np.mean(bert_scores['f1']),\n",
    "    \"FLESCH Readability Ease Score\": np.mean(flesch_scores),\n",
    "    \"Coleman-Liau Readability Score\": np.mean(coleman_liau_scores),\n",
    "    \"Perplexity Score\": np.mean(perplexity_scores),\n",
    "    \"Meteor Score\": np.mean(meteor_scores),\n",
    "    \"Mover Score\": np.mean(mover_scores),\n",
    "    \"Hamming Distance\": np.mean(hamming_distances),\n",
    "    \"Jaccard Similarity\": np.mean(jaccard_similarities)\n",
    "}\n",
    "\n",
    "summary_csv = f\"metrics_summary_blip2_{ABL}.csv\"\n",
    "with open(summary_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Metric\", \"Score\"])\n",
    "    for metric, score in metrics_summary.items():\n",
    "        writer.writerow([metric, score])\n",
    "print(f\"Metrics summary saved to {summary_csv}\")\n",
    "qualitative_data = []\n",
    "for idx in range(len(predicted_answers)):\n",
    "    qualitative_data.append({\n",
    "        \"video_id\": video_ids[idx],\n",
    "        \"predicted_answer\": predicted_answers[idx],\n",
    "        \"ground_truth\": ground_truths[idx],\n",
    "        \"ROUGE-1\": rouge_scores[\"rouge1\"][idx] if isinstance(rouge_scores[\"rouge1\"], list) else rouge_scores[\"rouge1\"],\n",
    "        \"ROUGE-2\": rouge_scores[\"rouge2\"][idx] if isinstance(rouge_scores[\"rouge2\"], list) else rouge_scores[\"rouge2\"],\n",
    "        \"ROUGE-L\": rouge_scores[\"rougeL\"][idx] if isinstance(rouge_scores[\"rougeL\"], list) else rouge_scores[\"rougeL\"],\n",
    "        \"BLEU-1\": bleu_scores[idx][0],\n",
    "        \"BLEU-2\": bleu_scores[idx][1],\n",
    "        \"BLEU-3\": bleu_scores[idx][2],\n",
    "        \"BERT Score\": bert_scores['f1'][idx] if isinstance(bert_scores['f1'], list) else bert_scores['f1'],\n",
    "        \"FLESCH Readability Ease Score\": flesch_scores[idx],\n",
    "        \"Coleman-Liau Readability Score\": coleman_liau_scores[idx],\n",
    "        \"Perplexity Score\": perplexity_scores[idx],\n",
    "        \"Meteor Score\": meteor_scores[idx],\n",
    "        \"Mover Score\": mover_scores[idx],\n",
    "        \"Hamming Distance\": hamming_distances[idx],\n",
    "        \"Jaccard Similarity\": jaccard_similarities[idx]\n",
    "    })\n",
    "\n",
    "sorted_qualitative_data = sorted(qualitative_data, key=lambda x: x[\"Mover Score\"], reverse=True)\n",
    "qualitative_csv = f\"qualitative_analysis_blip2_{ABL}.csv\"\n",
    "with open(qualitative_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=sorted_qualitative_data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(sorted_qualitative_data)\n",
    "\n",
    "print(f\"Qualitative analysis saved to {qualitative_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d9b2c-8930-4953-8ad1-52ccea440b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn.Functional as F\n",
    "import csv\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fddec0d0-8c35-44bc-b388-06fc35334964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59567aa389de46debab0beeb6446dddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "178f907c-f0e7-4c8a-8bd5-e6be6c657ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompt for chat format\n",
    "prompt = \"You are a legal assistant in India, answer the query. I have a tech comany which stores user data without permission, which indian cyber law and sections applicable to it will be applicable on me?. Give them in details, also provide any previous cases on these.\"\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Use .to(\"cuda\") for GPU inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bc26462-a47d-48c7-b17f-e9b43905e42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a legal assistant in India, answer the query. I have a tech comany which stores user data without permission, which indian cyber law and sections applicable to it will be applicable on me?. Give them in details, also provide any previous cases on these.\n",
      "\n",
      "My friend, you are in a bit of a sticky situation here. As a legal assistant in India, I must inform you that storing user data without permission is a violation of Indian cyber law and can lead to serious legal consequences.\n",
      "\n",
      "Under the Indian IT Act of 2000, Section 43(A) states that a body corporate, which is responsible for the violation of privacy of any person, shall be punishable with imprisonment for a term which may extend to three years, or with fine which may extend to one lakh rupees, or with both.\n",
      "\n",
      "Furthermore, the Indian Cyber Appellate Tribunal (ICAT) has established that the right to privacy is a fundamental right under the Indian Constitution, and any violation of this right can lead to legal action.\n",
      "\n",
      "In a landmark case, the Supreme Court of India in 2017, ruled that the right to privacy is a fundamental right under the Indian Constitution and is protected under Article 21.\n",
      "\n",
      "In addition, the General Data Protection Regulation (GDPR) of India also applies to tech companies that handle sensitive personal data of Indian citizens. The GDPR provides for strict penalties for non-compliance, including fines of up to €20 million or 4% of global annual turnover.\n",
      "\n",
      "In light of these legal provisions, I would advise your client to take immediate steps to obtain the necessary permissions from the users whose data they are storing, and to ensure that they comply with all applicable data protection laws.\n",
      "\n",
      "Previous cases have established that the unauthorized collection, use, and disclosure of personal data can result in significant legal and financial consequences. For instance, in 2018, the Data Protection Authority in India imposed a fine of Rs. 15,000 on a company for violating the IT Act by sharing personal data of its customers without their consent.\n",
      "\n",
      "In conclusion, your client's tech company must take the necessary steps to comply with Indian cyber law and data protection regulations to avoid any legal repercussions. Ignoring these requirements can lead to severe consequences, including criminal prosecution, fines, and loss of reputation.\n",
      "\n",
      "I hope this information is helpful, and I wish you and your client the best of luck in resolving this matter.\n"
     ]
    }
   ],
   "source": [
    "# Generate a response from the model\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1500,       # Limits the length of the response\n",
    "    temperature=0.7,          # Controls creativity (higher = more random)\n",
    "    top_p=0.9,                # Limits sampling to the most probable tokens\n",
    "    do_sample=True            # Enables sampling (for more varied output)\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vinayak)",
   "language": "python",
   "name": "vinayak"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
