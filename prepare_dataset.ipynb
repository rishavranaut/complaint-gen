{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e644d8c6-065f-4ce9-ba3c-52c0be8c4244",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gmflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgmflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgmflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgmflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GMFlow\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gmflow'"
     ]
    }
   ],
   "source": [
    "from gmflow.gmflow.gmflow import GMFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d195f-c4e9-4b5b-a399-31f9012de5f0",
   "metadata": {},
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78275283-7d28-45ef-8b84-d6fa9c0bca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd279dd-eacf-41a7-8655-1f9a4a0edadb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gmflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgmflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgmflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgmflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GMFlow\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict, Features, Image, Value, ClassLabel\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gmflow'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gmflow.gmflow.gmflow import GMFlow\n",
    "from datasets import Dataset, DatasetDict, Features, Image, Value, ClassLabel\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from tqdm import tqdm\n",
    "class VideoProcessor:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "\n",
    "    def load_models(self, gm_loc: str) -> None:\n",
    "        \"\"\"\n",
    "        Load GMFlow model for frame extraction.\n",
    "\n",
    "        Args:\n",
    "            gm_loc (str): Path to the model checkpoint for GMFlow.\n",
    "        \"\"\"\n",
    "        self.model = GMFlow(\n",
    "            feature_channels=128,\n",
    "            num_scales=1,\n",
    "            upsample_factor=8,\n",
    "            num_head=1,\n",
    "            attention_type=\"swin\",\n",
    "            ffn_dim_expansion=4,\n",
    "            num_transformer_layers=6,\n",
    "        ).to(self.device)\n",
    "        print(\"Loading GMFlow\")\n",
    "        checkpoint = torch.load(gm_loc, map_location=self.device)\n",
    "        weights = checkpoint[\"model\"] if \"model\" in checkpoint else checkpoint\n",
    "        self.model.load_state_dict(weights, strict=True)\n",
    "        self.model.eval()\n",
    "\n",
    "    def extract_relevant_frames_with_gmflow(self, video_loc, initial_frames_to_skip=4, initial_velocity_threshold=10, max_frames=3, min_frames=1):\n",
    "        \"\"\"\n",
    "        Extract relevant frames based on optical flow using GMFlow with dynamic frames_to_skip and velocity_threshold.\n",
    "    \n",
    "        Args:\n",
    "            video_loc (str): Path to the video file.\n",
    "            initial_frames_to_skip (int): Initial number of frames to skip between each comparison.\n",
    "            initial_velocity_threshold (float): Initial threshold to decide if the frame has significant motion.\n",
    "            max_frames (int): Maximum number of frames to extract.\n",
    "            min_frames (int): Minimum number of frames to extract.\n",
    "    \n",
    "        Returns:\n",
    "            List of tuples with relevant frames (images) and the frame number.\n",
    "        \"\"\"\n",
    "        video = cv.VideoCapture(video_loc)\n",
    "        good_frames = []\n",
    "        \n",
    "        total_frames = int(video.get(cv.CAP_PROP_FRAME_COUNT))\n",
    "        count = 0\n",
    "        \n",
    "        frames_to_skip = initial_frames_to_skip\n",
    "        velocity_threshold = initial_velocity_threshold\n",
    "    \n",
    "        while True:\n",
    "            ret1, frame1 = video.read()  \n",
    "            for _ in range(frames_to_skip):\n",
    "                ret2, frame2 = video.read()  \n",
    "            \n",
    "            if not ret1 or not ret2:\n",
    "                break\n",
    "            \n",
    "            # Resize and prepare frames for GMFlow\n",
    "            prev_frame = torch.from_numpy(cv.resize(frame1, (320, 160)).astype(np.float32)).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "            next_frame = torch.from_numpy(cv.resize(frame2, (320, 160)).astype(np.float32)).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                flow_results = self.model(prev_frame, next_frame, attn_splits_list=[2],\n",
    "                                          corr_radius_list=[-1],\n",
    "                                          prop_radius_list=[-1],\n",
    "                                          pred_bidir_flow=False)\n",
    "                flow = flow_results[\"flow_preds\"][-1]\n",
    "                velocity = torch.sqrt(flow[:, 0]**2 + flow[:, 1]**2)\n",
    "                mean_velocity = velocity.mean().item()\n",
    "            \n",
    "            if mean_velocity > velocity_threshold:\n",
    "                good_frames.append((count, frame2))  \n",
    "            \n",
    "            count += frames_to_skip\n",
    "    \n",
    "            if len(good_frames) < min_frames and frames_to_skip > 1:\n",
    "                frames_to_skip -= 1\n",
    "            elif len(good_frames) > max_frames and frames_to_skip < initial_frames_to_skip:\n",
    "                frames_to_skip += 1\n",
    "    \n",
    "            # Adjust velocity threshold to fine-tune frame selection\n",
    "            if len(good_frames) < min_frames:\n",
    "                velocity_threshold = max(velocity_threshold - 1, 1) \n",
    "            elif len(good_frames) > max_frames:\n",
    "                velocity_threshold += 1\n",
    "    \n",
    "        video.release()\n",
    "        \n",
    "        if len(good_frames) < min_frames:\n",
    "            video.set(cv.CAP_PROP_POS_FRAMES, total_frames - 1)\n",
    "            ret, last_frame = video.read()\n",
    "            if ret:\n",
    "                good_frames.append((total_frames - 1, last_frame))\n",
    "        \n",
    "        return good_frames[:max_frames]\n",
    "\n",
    "\n",
    "    def process_video_dataset(self, csv_file, videos_dir):\n",
    "        \"\"\"\n",
    "        Process the videos from the dataset and prepare relevant frames.\n",
    "\n",
    "        Args:\n",
    "            csv_file (str): CSV file containing the video names, questions, and answers.\n",
    "            videos_dir (str): Directory containing the video files.\n",
    "        \n",
    "        Returns:\n",
    "            Hugging Face dataset dictionary for uploading.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        data = []\n",
    "        \n",
    "        for index, row in tqdm(df.iterrows()):\n",
    "            video_name = row['ID']+\".mp4\"\n",
    "            video_path = os.path.join(videos_dir, video_name)\n",
    "\n",
    "            if not os.path.exists(video_path):\n",
    "                print(f\"Video file {video_path} does not exist, skipping.\")\n",
    "                continue\n",
    "            # print(f\"Processing video: {video_name}\")\n",
    "            relevant_frames = self.extract_relevant_frames_with_gmflow(video_path)\n",
    "            # print(len(relevant_frames))\n",
    "            resize_transform = transforms.Resize((360,360))\n",
    "            for frame_num, frame in relevant_frames:\n",
    "                question = \"<image>\\nWhat complaint is conveyed by the user in this image?\"\n",
    "                answer = row['Label']\n",
    "                if isinstance(frame, np.ndarray):\n",
    "                    frame = PILImage.fromarray(cv.cvtColor(frame, cv.COLOR_BGR2RGB)) \n",
    "                frame=resize_transform(frame)\n",
    "                f2=Image()\n",
    "                frame=f2.encode_example(frame)\n",
    "                data.append({\n",
    "                    \"image\": frame,  \n",
    "                    \"ID\": video_name+str(frame_num),\n",
    "                    \"question\": question,\n",
    "                    \"answer\": answer\n",
    "                })\n",
    "        features = Features({\n",
    "            \"image\": Image(), \n",
    "            \"ID\": Value(\"string\"),\n",
    "            \"question\": Value(\"string\"),\n",
    "            \"answer\": Value(\"string\")\n",
    "        })\n",
    "        return Dataset.from_pandas(pd.DataFrame(data), features=features)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "683f17fc-118b-4265-81b4-b7d53b3e98a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GMFlow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [09:58,  3.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image as PILImage\n",
    "video_processor = VideoProcessor(\"cuda\")\n",
    "gmflow_checkpoint = \"gmflow/pretrained/gmflow_sintel-0c07dcb3.pth\"  # Path to the GMFlow checkpoint\n",
    "video_processor.load_models(gmflow_checkpoint)\n",
    "csv_file = '/home/sarmistha/Testing2/Copy of New_200_text - Sheet1.csv'\n",
    "videos_dir = '/home/sarmistha/Research/videos'\n",
    "dataset = video_processor.process_video_dataset(csv_file, videos_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2763ff6-e15b-4057-9adf-6cfa9b7b6b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44bc00414b1410ab28f29d367366048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73a6e93d3824c7786d7e8332ddafc2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/411 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deab020218ee416cbc7c28197fa26812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cerelac2/consumer-complaint-vqa/commit/def97b9531b5e73c50f67bf180449857d2b1d527', commit_message='Upload dataset', commit_description='', oid='def97b9531b5e73c50f67bf180449857d2b1d527', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_repo_id = \"cerelac2/consumer-complaint-vqa\"\n",
    "hf_token = \"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\" \n",
    "dataset.push_to_hub(hf_repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e94d6d-00b1-4693-955e-9ea79daa4dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a7a070-2c37-4b2c-bc24-6924a3cefd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering ds2 for train split matches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ds2: 100%|██████████| 1280/1280 [00:07<00:00, 179.95it/s]\n",
      "Processing ds2: 100%|██████████| 160/160 [00:00<00:00, 182.45it/s]\n",
      "Processing ds2: 100%|██████████| 160/160 [00:01<00:00, 137.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "\n",
    "# Load ds1 and ds2 datasets\n",
    "ds1 = load_dataset(\"zera09/Video_complaint\")\n",
    "ds2 = load_dataset(\"cerelac2/consumer-complaint-vqa\")\n",
    "\n",
    "# Extract unique video names from ds1 train and test\n",
    "train_video_names = set(row['video'] for row in ds1['train'])\n",
    "test_video_names = set(row['video'] for row in ds1['test'])\n",
    "\n",
    "# Initialize lists to store new train and test samples\n",
    "new_train = []\n",
    "new_test = []\n",
    "\n",
    "# Function to check if an ID matches any video name in a set\n",
    "def matches_video(video_names, id):\n",
    "    return any(id.startswith(video) for video in video_names)\n",
    "\n",
    "# Process ds2 splits all at once, filtering based on video names\n",
    "print(\"Filtering ds2 for train split matches...\")\n",
    "for row in tqdm(ds2['train'], desc=\"Processing ds2\"):\n",
    "    if matches_video(train_video_names, row['ID']):\n",
    "        new_train.append(row)\n",
    "    elif matches_video(test_video_names, row['ID']):\n",
    "        new_test.append(row)\n",
    "for row in tqdm(ds2['test'], desc=\"Processing ds2\"):\n",
    "    if matches_video(train_video_names, row['ID']):\n",
    "        new_train.append(row)\n",
    "    elif matches_video(test_video_names, row['ID']):\n",
    "        new_test.append(row)\n",
    "for row in tqdm(ds2['validation'], desc=\"Processing ds2\"):\n",
    "    if matches_video(train_video_names, row['ID']):\n",
    "        new_train.append(row)\n",
    "    elif matches_video(test_video_names, row['ID']):\n",
    "        new_test.append(row)\n",
    "# Convert lists to Dataset format\n",
    "new_train_dataset = Dataset.from_list(new_train)\n",
    "new_test_dataset = Dataset.from_list(new_test)\n",
    "\n",
    "# Create a new DatasetDict\n",
    "new_dataset = DatasetDict({\n",
    "    \"train\": new_train_dataset,\n",
    "    \"test\": new_test_dataset\n",
    "})\n",
    "\n",
    "# Push new dataset to the Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52aceb0b-e519-4b08-a34f-e86822ff5f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'ID', 'question', 'answer', 'emotion', 'sentiment', 'sarcasm'],\n",
       "        num_rows: 1355\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'ID', 'question', 'answer', 'emotion', 'sentiment', 'sarcasm'],\n",
       "        num_rows: 245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6d21e59-67fd-47c8-b8f4-7375a90e02d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee9fe14203c4f529ec77c0054283cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4108e1ceda34dababe29af9e28e03ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1355 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf54ca02728744dabc8997216fc43520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02a3825ca648a69159c64ab2d3c9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb50133315e24e0c9708f327f5421249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83e9efe432b4843a29a10a065d5ffd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/cerelac2/consumer-complaint-vqa/commit/441c28d7446789a3e57e3384e47e868b5a45045a', commit_message='Upload dataset', commit_description='', oid='441c28d7446789a3e57e3384e47e868b5a45045a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "new_dataset.push_to_hub(\"cerelac2/consumer-complaint-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbd99e7-fa36-4488-ab07-1f3bae8c1489",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vllm2)",
   "language": "python",
   "name": "vllm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
