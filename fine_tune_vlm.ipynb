{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/DATA/rishav_2311mc12/anaconda3/envs/codeMix/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoProcessor, BitsAndBytesConfig, Idefics3ForConditionalGeneration\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "import re\n",
    "\n",
    "import logging\n",
    "import random\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from datasets import Dataset, Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n",
      "INFO:peft.tuners.tuners_utils:Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10536960, 2256809840)\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed) # if you are using cuda\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    np.random.seed(seed) # Numpy module.\n",
    "    random.seed(seed) # Python random module.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed()\n",
    "\n",
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "SMOL = True\n",
    "\n",
    "model_id = \"HuggingFaceTB/SmolVLM-Instruct\" if SMOL else \"HuggingFaceM4/Idefics3-8B-Llama3\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id\n",
    ")\n",
    "\n",
    "if USE_QLORA or USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],\n",
    "        use_dora=False if USE_QLORA else True,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        # task_type=TaskType.\n",
    "    )\n",
    "    lora_config.inference_mode = False\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.add_adapter(lora_config)\n",
    "    model.enable_adapters()\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(model.get_nb_trainable_parameters())\n",
    "else:\n",
    "    model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map='auto'\n",
    "    )\n",
    "\n",
    "    # if you'd like to only fine-tune LLM\n",
    "    for param in model.model.vision_model.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params_with_grad(model):\n",
    "    \"\"\"\n",
    "    Checks and prints the parameters of a PyTorch model that have requires_grad=True.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to check.\n",
    "    \"\"\"\n",
    "\n",
    "    params_with_grad = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            params_with_grad.append((name, param))\n",
    "\n",
    "    if params_with_grad:\n",
    "        print(\"Parameters with requires_grad=True:\")\n",
    "        for name, param in params_with_grad:\n",
    "            print(f\"- {name}: shape={param.shape}, dtype={param.dtype}, params={param}\")\n",
    "            break\n",
    "            # Optionally print the number of elements:\n",
    "            print(f\"  Number of elements: {param.numel()}\")\n",
    "    else:\n",
    "        print(\"No parameters with requires_grad=True found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters with requires_grad=True:\n",
      "- base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 1152]), dtype=torch.float32, params=Parameter containing:\n",
      "tensor([[ 0.0562, -0.1976, -0.0880,  ...,  0.0183, -0.1876, -0.0010],\n",
      "        [-0.0583,  0.0611, -0.2928,  ...,  0.0292,  0.1838, -0.0823],\n",
      "        [ 0.3044,  0.0196,  0.2708,  ...,  0.0573, -0.1446,  0.0779],\n",
      "        ...,\n",
      "        [-0.1374, -0.0511,  0.0835,  ...,  0.0658, -0.2263, -0.3412],\n",
      "        [-0.2727, -0.0401,  0.0655,  ...,  0.0611, -0.2045,  0.0332],\n",
      "        [ 0.0171,  0.0390, -0.1298,  ...,  0.0427,  0.0269,  0.3102]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Parameters with requires_grad=True:\\n- base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 1152]), dtype=torch.float32, params=Parameter containing:\\ntensor([[ 0.0562, -0.1976, -0.0880,  ...,  0.0183, -0.1876, -0.0010],\\n        [-0.0583,  0.0611, -0.2928,  ...,  0.0292,  0.1838, -0.0823],\\n        [ 0.3044,  0.0196,  0.2708,  ...,  0.0573, -0.1446,  0.0779],\\n        ...,\\n        [-0.1374, -0.0511,  0.0835,  ...,  0.0658, -0.2263, -0.3412],\\n        [-0.2727, -0.0401,  0.0655,  ...,  0.0611, -0.2045,  0.0332],\\n        [ 0.0171,  0.0390, -0.1298,  ...,  0.0427,  0.0269,  0.3102]],\\n       device='cuda:0', requires_grad=True)\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_params_with_grad(model)\n",
    "'''Parameters with requires_grad=True:\n",
    "- base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 1152]), dtype=torch.float32, params=Parameter containing:\n",
    "tensor([[ 0.0562, -0.1976, -0.0880,  ...,  0.0183, -0.1876, -0.0010],\n",
    "        [-0.0583,  0.0611, -0.2928,  ...,  0.0292,  0.1838, -0.0823],\n",
    "        [ 0.3044,  0.0196,  0.2708,  ...,  0.0573, -0.1446,  0.0779],\n",
    "        ...,\n",
    "        [-0.1374, -0.0511,  0.0835,  ...,  0.0658, -0.2263, -0.3412],\n",
    "        [-0.2727, -0.0401,  0.0655,  ...,  0.0611, -0.2045,  0.0332],\n",
    "        [ 0.0171,  0.0390, -0.1298,  ...,  0.0427,  0.0269,  0.3102]],\n",
    "       device='cuda:0', requires_grad=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the CSV.\n",
    "df = pd.read_csv(\"/DATA/rishav_2311mc12/complaint_gen2/Gender-Extension of Complaint Generation model - Sheet1.csv\")\n",
    "df = df[['Video Link', 'Gender', 'Age-Group', 'Our Label']]  # Select necessary columns\n",
    "df.dropna(inplace=True)  # Drop rows with missing values\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Add the prompt column\n",
    "df['prompt'] = df.apply(\n",
    "    lambda row: f\"Given a video made by {row['Gender']} aged {row['Age-Group']}, generate a complaint in Hinglish Language about their experience from their perspective of the given product.\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# Rename columns to match the features schema\n",
    "df.rename(columns={\n",
    "    'Video Link': 'video_path',\n",
    "    'Gender': 'gender',\n",
    "    'Age-Group': 'age_group',\n",
    "    'Our Label': 'label'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Define features for the Dataset\n",
    "features = Features({\n",
    "    'video_path': Value(dtype='string'),\n",
    "    'gender': Value(dtype='string'),\n",
    "    'age_group': Value(dtype='string'),\n",
    "    'label': Value(dtype='string'),\n",
    "    'prompt': Value(dtype='string'),\n",
    "})\n",
    "\n",
    "\n",
    "# Create Hugging Face Dataset\n",
    "my_dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Perform train-test split\n",
    "split_dataset = my_dataset.train_test_split(test_size=0.3)  # Adjust test_size as needed\n",
    "\n",
    "# Access train and test datasets\n",
    "train_ds = split_dataset['train']\n",
    "test_ds = split_dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path: str, max_frames: int) -> List[Image.Image]:\n",
    "    \"\"\"\n",
    "    Extract frames from a video file. The video must already exist locally.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        max_frames (int): Maximum number of frames to extract.\n",
    "    \n",
    "    Returns:\n",
    "        List[Image.Image]: A list of PIL Image objects extracted from the video.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Warning: Could not locate video: {video_path}. Skipping...\")\n",
    "        return []  # Return an empty list for the frames\n",
    "\n",
    "    # Open the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video: {video_path}. Skipping...\")\n",
    "        return []  # Return an empty list for the frames\n",
    "\n",
    "    # Get video properties\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Calculate frame indices to extract (1 fps)\n",
    "    frame_indices = list(range(0, total_frames, fps))\n",
    "\n",
    "    # Sample evenly if more frames than max_frames\n",
    "    if len(frame_indices) > max_frames:\n",
    "        indices = np.linspace(0, len(frame_indices) - 1, max_frames, dtype=int)\n",
    "        frame_indices = [frame_indices[i] for i in indices]\n",
    "\n",
    "    frames = []\n",
    "    for frame_idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_image = Image.fromarray(frame)\n",
    "            frames.append(pil_image)\n",
    "\n",
    "    cap.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds['video_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def get_video_id_from_path(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the last ID (e.g., B0DBJ4SZHW) from the video path (Amazon review URL or any other identifier).\n",
    "    Assumes video path contains the product ID or video ID in a known format.\n",
    "    \"\"\"\n",
    "    # Assuming the video path format has the ID at the end of the URL or as part of the name\n",
    "    match = re.search(r'ASIN=([A-Z0-9]{10})', video_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    max_frames = 8\n",
    "    for example in examples:\n",
    "        # Extract the video ID from the video path\n",
    "        video_path = example[\"video_path\"]\n",
    "        video_id = get_video_id_from_path(video_path)\n",
    "        \n",
    "        if video_id is None:\n",
    "            print(f\"Error: No valid video ID found in {video_path}. Skipping...\")\n",
    "            continue  # Skip this example if no valid ID is found\n",
    "\n",
    "        # Define the path to the downloaded video (expected to be in the \"downloads\" folder)\n",
    "        download_folder = \"/DATA/rishav_2311mc12/complaint_gen2/downloads\"\n",
    "        video_file_path = os.path.join(download_folder, f\"{video_id}.mp4\")\n",
    "\n",
    "        # Check if the video already exists in the download folder\n",
    "        if not os.path.exists(video_file_path):\n",
    "            print(f\"Error: Video with ID {video_id} not found in {download_folder}. Skipping...\")\n",
    "            continue  # Skip this example if the video file does not exist\n",
    "\n",
    "        # Extract frames from the video\n",
    "        frames = extract_frames(video_file_path, max_frames)  # Returns a list of PIL images\n",
    "\n",
    "        prompt = example[\"prompt\"]\n",
    "        label = example[\"label\"]\n",
    "\n",
    "        # Prepare messages - Repeat <image> token for each frame\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    # Repeat <image> for each frame\n",
    "                    *[{\"type\": \"image\"} for _ in range(len(frames))],\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": label}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=False)\n",
    "\n",
    "        # Add processed data\n",
    "        texts.append(text.strip())\n",
    "        images.extend(frames)  # Add frames for the video\n",
    "\n",
    "    # Create the batch using the processor\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Prepare labels\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [90/90 20:46, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.412300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = model_id.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    optim=\"paged_adamw_8bit\", # for 8-bit, keep this, else adamw_hf\n",
    "    bf16=True, # underlying precision for 8bit\n",
    "    output_dir=f\"./{model_name}-codemix\",\n",
    "    hub_model_id=f\"{model_name}-codemix\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_ds,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors:   0%|          | 0.00/42.2M [00:00<?, ?B/s]\n",
      "\u001b[A\n",
      "\n",
      "adapter_model.safetensors:   0%|          | 164k/42.2M [00:00<01:18, 537kB/s]  \n",
      "training_args.bin: 100%|██████████| 5.30k/5.30k [00:01<00:00, 3.77kB/s].82MB/s]\n",
      "events.out.tfevents.1735941011.user.1612814.0: 100%|██████████| 13.3k/13.3k [00:01<00:00, 9.35kB/s]\n",
      "adapter_model.safetensors: 100%|██████████| 42.2M/42.2M [00:08<00:00, 4.73MB/s]\n",
      "\n",
      "\n",
      "Upload 3 LFS files: 100%|██████████| 3/3 [00:09<00:00,  3.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rishavranaut/SmolVLM-Instruct-codemix/commit/bd0b5605a2031cda825c9f8e3576d4dc5fdcf3df', commit_message='End of training', commit_description='', oid='bd0b5605a2031cda825c9f8e3576d4dc5fdcf3df', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rishavranaut/SmolVLM-Instruct-codemix', endpoint='https://huggingface.co', repo_type='model', repo_id='rishavranaut/SmolVLM-Instruct-codemix'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parameters with requires_grad=True found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Parameters with requires_grad=True:\\n- base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 1152]), dtype=torch.float32, params=Parameter containing:\\ntensor([[ 0.0562, -0.1976, -0.0880,  ...,  0.0183, -0.1876, -0.0010],\\n        [-0.0583,  0.0611, -0.2928,  ...,  0.0292,  0.1838, -0.0823],\\n        [ 0.3044,  0.0196,  0.2708,  ...,  0.0573, -0.1446,  0.0779],\\n        ...,\\n        [-0.1374, -0.0511,  0.0835,  ...,  0.0658, -0.2263, -0.3412],\\n        [-0.2727, -0.0401,  0.0655,  ...,  0.0611, -0.2045,  0.0332],\\n        [ 0.0171,  0.0390, -0.1298,  ...,  0.0427,  0.0269,  0.3102]],\\n       device='cuda:0', requires_grad=True)\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_params_with_grad(model)\n",
    "'''Parameters with requires_grad=True:\n",
    "- base_model.model.model.vision_model.encoder.layers.0.self_attn.k_proj.lora_A.default.weight: shape=torch.Size([8, 1152]), dtype=torch.float32, params=Parameter containing:\n",
    "tensor([[ 0.0562, -0.1976, -0.0880,  ...,  0.0183, -0.1876, -0.0010],\n",
    "        [-0.0583,  0.0611, -0.2928,  ...,  0.0292,  0.1838, -0.0823],\n",
    "        [ 0.3044,  0.0196,  0.2708,  ...,  0.0573, -0.1446,  0.0779],\n",
    "        ...,\n",
    "        [-0.1374, -0.0511,  0.0835,  ...,  0.0658, -0.2263, -0.3412],\n",
    "        [-0.2727, -0.0401,  0.0655,  ...,  0.0611, -0.2045,  0.0332],\n",
    "        [ 0.0171,  0.0390, -0.1298,  ...,  0.0427,  0.0269,  0.3102]],\n",
    "       device='cuda:0', requires_grad=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: image_seq_len. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Idefics3ForConditionalGeneration(\n",
       "      (model): Idefics3Model(\n",
       "        (vision_model): Idefics3VisionTransformer(\n",
       "          (embeddings): Idefics3VisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "            (position_embedding): Embedding(729, 1152)\n",
       "          )\n",
       "          (encoder): Idefics3Encoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-26): 27 x Idefics3EncoderLayer(\n",
       "                (self_attn): Idefics3VisionAttention(\n",
       "                  (k_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_proj): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                (mlp): Idefics3VisionMLP(\n",
       "                  (activation_fn): PytorchGELUTanh()\n",
       "                  (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "                  (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "        (connector): Idefics3Connector(\n",
       "          (modality_projection): Idefics3SimpleMLP(\n",
       "            (proj): Linear(in_features=10368, out_features=2048, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (text_model): LlamaModel(\n",
       "          (embed_tokens): Embedding(49155, 2048, padding_idx=2)\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaSdpaAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=49155, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "path = \"rishavranaut/SmolVLM-Instruct-codemix\"\n",
    "config = PeftConfig.from_pretrained(path)\n",
    "model1 = Idefics3ForConditionalGeneration.from_pretrained(config.base_model_name_or_path,device_map='auto')\n",
    "model = PeftModel.from_pretrained(model1, path)\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    config.base_model_name_or_path\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracted 3 frames from video\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Given a video made by Male aged middle Aged, generate a complaint in Hinglish Language about their experience from their perspective of the given product.\n",
      "Response: User:<image>Given a video made by Male aged middle Aged, generate a complaint in Hinglish Language about their experience from their perspective of the given product.\n",
      "Assistant: Video mein Male aged middle Aged dikhaya gaya hai, jiske liye Samsung Galaxy A5 (2017) dikhaya gaya hai. Video mein yeh Samsung Galaxy A5 (2017) dikhaya gaya hai, jahan yeh user dikhata hai, jo yeh Samsung Galaxy A5 (2017) dikhata hai. Yeh Samsung Galaxy A5 (2\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "import os\n",
    "image_token_id = processor.tokenizer.additional_special_tokens_ids[\n",
    "            processor.tokenizer.additional_special_tokens.index(\"<image>\")]\n",
    "\n",
    "def get_video_id_from_path(video_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the last ID (e.g., B0DBJ4SZHW) from the video path (Amazon review URL or any other identifier).\n",
    "    Assumes video path contains the product ID or video ID in a known format.\n",
    "    \"\"\"\n",
    "    # Assuming the video path format has the ID at the end of the URL or as part of the name\n",
    "    match = re.search(r'ASIN=([A-Z0-9]{10})', video_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "def get_video_path(example):\n",
    "\n",
    "    video_path = example\n",
    "    video_id = get_video_id_from_path(video_path)\n",
    "\n",
    "    # Define the path to the downloaded video (expected to be in the \"downloads\" folder)\n",
    "    download_folder = \"/DATA/rishav_2311mc12/complaint_gen2/downloads\"\n",
    "    video_file_path = os.path.join(download_folder, f\"{video_id}.mp4\")\n",
    "    return video_file_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_response(model, processor, video_path: str, prompt: str, max_frames: int = 3):\n",
    "    # Extract frames\n",
    "    frames  = extract_frames(video_path,max_frames)\n",
    "    \n",
    "    logger.info(f\"Extracted {len(frames)} frames from video\")\n",
    "    \n",
    "    # Create prompt with frames\n",
    "    prompt = prompt\n",
    "\n",
    "    # Prepare messages - Repeat <image> token for each frame\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                # Repeat <image> for each frame\n",
    "                *[{\"type\": \"image\"} for _ in range(len(frames))],\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Process inputs\n",
    "    inputs = processor(\n",
    "        text=processor.apply_chat_template(messages, add_generation_prompt=True),\n",
    "        images=[img for img in frames],\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "     \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        use_cache=True\n",
    "    )\n",
    "    # Decode response\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "def main(): \n",
    "    video_path = video_path = get_video_path(test_ds['video_path'][0])\n",
    "    prompt = test_ds['prompt'][0]\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "  \n",
    "    # Generate response\n",
    "    logger.info(\"Generating response...\")\n",
    "    response = generate_response(model, processor, video_path, prompt)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Question:\", prompt)\n",
    "    print(\"Response:\", response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder, create_repo\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory.\n",
    "output_dir = f\"./{model_name}-codemix\"\n",
    "repo_name = \"smolvlm-fine-tuned-codemix\"\n",
    "\n",
    "repo_id = f\"rishavranaut/{repo_name}\"\n",
    "\n",
    "\n",
    "repo_id = create_repo(repo_id, exist_ok=True).repo_id\n",
    "\n",
    "upload_folder(\n",
    "    repo_id=repo_id,\n",
    "    folder_path=output_dir,\n",
    "    commit_message=\"Pushed the IDEFICS2 fine-tuned model.\",\n",
    "    ignore_patterns=[\"step_*\", \"epoch_*\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "base_model = Idefics3ForConditionalGeneration.from_pretrained(\n",
    "        model_id=\"HuggingFaceTB/SmolVLM-Base\",\n",
    "        quantization_config=bnb_config if USE_QLORA else None,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "model = PeftModel.Idefics3ForConditionalGeneration(base_model, \"/DATA/rishav_2311mc12/complaint_gen2/SmolVLM-Base-codemix\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input messages\n",
    "\n",
    "video_path = get_video_path(test_ds['video_path'][0])\n",
    "prompt = test_ds['prompt'][0]\n",
    "frames = extract_frames(video_path,max_frames=8)\n",
    "messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    # Repeat <image> for each frame\n",
    "                    *[{\"type\": \"image\"} for _ in range(len(frames))],\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# Prepare inputs\n",
    "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "inputs = processor(text=prompt, images=frames, return_tensors=\"pt\")\n",
    "\n",
    "# Generate outputs\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "\n",
    "print(generated_texts[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "video downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df= pd.read_csv(\"/DATA/rishav_2311mc12/complaint_gen2/Gender-Extension of Complaint Generation model - Sheet1.csv\")\n",
    "# df.dropna(inplace=True)\n",
    "# df\n",
    "\n",
    "\n",
    "# import os\n",
    "# import re\n",
    "# from yt_dlp import YoutubeDL\n",
    "\n",
    "# def download_video(video_url, download_path):\n",
    "#     \"\"\"\n",
    "#     Download a video and save it using the product ID as filename.\n",
    "    \n",
    "#     Args:\n",
    "#         video_url (str): URL of the video to download.\n",
    "#         download_path (str): Path where the video will be saved.\n",
    "#     \"\"\"\n",
    "#     # Extract product ID from the URL (assuming it's in the format: ...?ASIN=PRODUCT_ID)\n",
    "#     product_id = re.search(r'ASIN=([A-Z0-9]{10})', video_url)\n",
    "#     if product_id:\n",
    "#         product_id = product_id.group(1)\n",
    "#     else:\n",
    "#         print(\"Could not extract product ID.\")\n",
    "#         return\n",
    "    \n",
    "#     # Ensure the download directory exists\n",
    "#     if not os.path.exists(download_path):\n",
    "#         os.makedirs(download_path)\n",
    "    \n",
    "#     # Configure yt-dlp options to save video with the product ID as the filename\n",
    "#     ydl_opts = {\n",
    "#         'outtmpl': os.path.join(download_path, f'{product_id}.%(ext)s'),\n",
    "#         'format': 'best',\n",
    "#     }\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Downloading: {video_url}\")\n",
    "#         with YoutubeDL(ydl_opts) as ydl:\n",
    "#             ydl.download([video_url])\n",
    "#         print(f\"Downloaded: {product_id}.mp4\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to download {video_url}: {e}\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Replace with actual video URLs (assuming these are the review video links)\n",
    "#     video_links = df['Video Link']\n",
    "    \n",
    "#     # Specify the download folder\n",
    "#     download_folder = \"/DATA/rishav_2311mc12/complaint_gen2/downloads\"\n",
    "    \n",
    "#     # Loop through the video URLs and download each one\n",
    "#     for video_url in video_links:\n",
    "#         download_video(video_url, download_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
