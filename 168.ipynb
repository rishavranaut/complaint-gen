{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbe36a0f-0fa4-410a-815a-ff694b2d4cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/sarmistha/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bb2c9d-c8d6-4cb3-9af8-89d5f890995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import VideoLlavaForConditionalGeneration, VideoLlavaProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import av\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def create_and_prepare_model(model_name):\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "    model = VideoLlavaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config = quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"out_proj\"],#['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, config)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Helper function to check if video file exists\n",
    "def is_video_available(video_path):\n",
    "    \n",
    "    return os.path.exists(video_path)\n",
    "    container = av.open(video_path)\n",
    "    video = container.streams.get(0)[0]\n",
    "\n",
    "    av_timestamps = [\n",
    "        int(packet.pts * video.time_base) for packet in container.demux(video) if packet.pts is not None\n",
    "    ]\n",
    "\n",
    "    av_timestamps.sort()\n",
    "    start_id = bisect.bisect_left(av_timestamps, start)\n",
    "    end_id = bisect.bisect_left(av_timestamps, end)\n",
    "\n",
    "    # in case it is a very short video, lets take a longer duration and sample\n",
    "    if end_id  - start_id < 10:\n",
    "        end_id += 10\n",
    "        start_id -= 10\n",
    "\n",
    "    end_id = min(len(av_timestamps) - 1, end_id)\n",
    "    start_id = max(1, start_id)\n",
    "\n",
    "    # We sample 8 frames for tuning following the original paper\n",
    "    # But we can increase the number of frames for longer videos and check out if it helps performance\n",
    "    # Change the below \"8\" to any number of frames you want, and note that more frames -> more computational resources needed\n",
    "    indices = np.linspace(start_id, end_id, 8).astype(int)\n",
    "\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_id:\n",
    "            break\n",
    "        if i >= start_id and i in indices:\n",
    "            frames.append(frame)\n",
    "    assert len(frames) == 8, f\"Got {len(frames)} frames but should be 8. Check the indices: {indices};, start_id: {start_id}, end_id: {end_id}. Len of video is {len(av_timestamps)} frames.\"\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    # print(container.decode(video=0))\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        x = indices.count(i)\n",
    "        if i > end_index:\n",
    "            break\n",
    "        for _ in range(x):# if x == 0, is not happend anyway\n",
    "            frames.append(frame)\n",
    "    while len(frames) < len(indices):\n",
    "        frames.append(frames[-1])\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "class VideoTextDataset(Dataset):\n",
    "    def __init__(self, video_dir, df, num_frames=8, max_length=512):\n",
    "        self.video_dir = video_dir\n",
    "        self.df = df\n",
    "        self.processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n",
    "        self.num_frames = num_frames\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        video_file = row['ID'] + '.mp4'\n",
    "        label = row['Label']\n",
    "        video_path = os.path.join(self.video_dir, video_file)\n",
    "        container = av.open(video_path)\n",
    "        total_frames = container.streams.video[0].frames\n",
    "        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        video = read_video_pyav(container, list(indices))\n",
    "        # print(video.shape)\n",
    "        #prompt = f\"USER: <video>What complaint is conveyed by the user in the video? ASSISTANT: {label}\"\n",
    "        #inputs = self.processor(text=prompt, videos=video, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_length, truncation=True)\n",
    "        prompt = \"USER: <video>What complaint is conveyed by the user in the video? ASSISTANT:\"\n",
    "        \n",
    "        inputs = self.processor(text=prompt, videos=video, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_length, truncation=True)\n",
    "        labels = self.processor(text=label, return_tensors=\"pt\", padding=\"max_length\", max_length=self.max_length, truncation=True)['input_ids'].squeeze(0)\n",
    "        \n",
    "        # Remove the extra batch dimension added by the processor\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.squeeze(0)\n",
    "        \n",
    "        inputs['labels'] = labels\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "        pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'pixel_values': pixel_values\n",
    "        }\n",
    "    # def collate_read_video(example, path):\n",
    "    # # Some datasets have a start-end interval, so we try to get it if exists. Otherwise just set a very large end timestamp\n",
    "    # clip = read_video_pyav(f'{path}/{example[\"video\"]}', example.get(\"start\", 1), example.get(\"end\", 1e+10))\n",
    "    # example[\"clip\"] = clip\n",
    "    # return example\n",
    "\n",
    "def load_and_filter_dataset(csv_file, video_dir):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    # Add a column to check if the video file exists\n",
    "    df['video_path'] = df['ID'].apply(lambda x: os.path.join(video_dir, x+'.mp4'))\n",
    "    df['exists'] = df['video_path'].apply(is_video_available)\n",
    "    # Filter out rows where video files do not exist\n",
    "    filtered_df = df[df['exists']].copy()\n",
    "    # Remove the 'exists' column as it was just for filtering\n",
    "    filtered_df.drop(columns=['exists'], inplace=True)\n",
    "    print(f\"Loaded {len(df)} entries, {len(filtered_df)} valid video entries after filtering.\")\n",
    "    return filtered_df\n",
    "\n",
    "def load_and_split_data(csv_file, video_dir, train_size=0.8, val_size=0.1, test_size=0.1):\n",
    "    # Load and filter dataset\n",
    "    df = load_and_filter_dataset(csv_file, video_dir)\n",
    "    # First split: separate test set\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n",
    "    # Second split: separate train and validation from the remaining data\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_size/(train_size + val_size), random_state=42)\n",
    "    print(f\"Dataset splits: Train {len(train_df)}, Validation {len(val_df)}, Test {len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119164a9-4b96-4dd4-b545-eb771d181505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2eae6b28fc421a9c66399187d9acd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"LanguageBind/Video-LLaVA-7B-hf\"\n",
    "# Create and prepare the model\n",
    "model = create_and_prepare_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b73bfc-9067-4198-8bfd-97498ea21d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): VideoLlavaForConditionalGeneration(\n",
       "      (video_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (image_tower): CLIPVisionModel(\n",
       "        (vision_model): CLIPVisionTransformer(\n",
       "          (embeddings): CLIPVisionEmbeddings(\n",
       "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "            (position_embedding): Embedding(257, 1024)\n",
       "          )\n",
       "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder): CLIPEncoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-23): 24 x CLIPEncoderLayer(\n",
       "                (self_attn): CLIPSdpaAttention(\n",
       "                  (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (q_proj): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  (out_proj): Linear4bit(\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=True)\n",
       "                  )\n",
       "                )\n",
       "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (mlp): CLIPMLP(\n",
       "                  (activation_fn): QuickGELUActivation()\n",
       "                  (fc1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "                  (fc2): Linear4bit(in_features=4096, out_features=1024, bias=True)\n",
       "                )\n",
       "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (multi_modal_projector): VideoLlavaMultiModalProjector(\n",
       "        (linear_1): Linear4bit(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELUActivation()\n",
       "        (linear_2): Linear4bit(in_features=4096, out_features=4096, bias=True)\n",
       "      )\n",
       "      (language_model): LlamaForCausalLM(\n",
       "        (model): LlamaModel(\n",
       "          (embed_tokens): Embedding(32064, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x LlamaDecoderLayer(\n",
       "              (self_attn): LlamaSdpaAttention(\n",
       "                (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (rotary_emb): LlamaRotaryEmbedding()\n",
       "              )\n",
       "              (mlp): LlamaMLP(\n",
       "                (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "                (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3580ff-a6d6-4166-b4d5-c7c88bc25027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def export_to_json(df, video_dir, output_json):\n",
    "    data = []\n",
    "    for idx, row in df.iterrows():\n",
    "        video_file = row['ID'] + '.mp4'\n",
    "        label = row['Label']  # Adjust this if it's more than a single label\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        conversation = [\n",
    "            {\"from\": \"human\", \"value\": \"<video>\\nWhat complaint is conveyed by the user in the video?\"},\n",
    "            {\"from\": \"gpt\", \"value\": label}\n",
    "        ]\n",
    "        data.append({\n",
    "            \"id\": idx,\n",
    "            \"video\": video_path,\n",
    "            \"conversations\": conversation\n",
    "        })\n",
    "\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c3ba3ea-5619-4015-9948-d546afcf8e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 168 entries, 168 valid video entries after filtering.\n",
      "Dataset splits: Train 134, Validation 17, Test 17\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataset with real videos\n",
    "video_dir = '/home/sarmistha/Research/videos'\n",
    "csv_file = '/home/sarmistha/Testing2/Copy of New_200_text - Sheet1.csv'\n",
    "    \n",
    "\n",
    "# Load and split the data\n",
    "train_df, val_df, test_df = load_and_split_data(csv_file, video_dir)\n",
    "export_to_json(train_df, '/home/sarmistha/Testing2/vl2/VideoLLaMA2/datasets/custom_sft/videos/', 'custom.json')\n",
    "\n",
    "# # Create Dataset objects for each split\n",
    "train_dataset = VideoTextDataset(video_dir, train_df)\n",
    "val_dataset = VideoTextDataset(video_dir, val_df)\n",
    "test_dataset = VideoTextDataset(video_dir, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878a95a0-6c24-4f96-949d-103923d742f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"./New_video_llava_qlora\",\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        save_steps=20,\n",
    "        eval_steps=10,\n",
    "        logging_steps=1,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        push_to_hub=False,\n",
    "        load_best_model_at_end=False,\n",
    "        optim='paged_adamw_32bit',\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c6d5fa-befd-4d18-ba1e-79468d8b3adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=lambda data: {key: torch.stack([example[key] for example in data]) for key in data[0]},\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07adaf85-1ab5-461e-a2e8-4bb931e552a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. It is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-07 18:52:29,904] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeramarveenlyngkhoi\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sarmistha/Testing2/wandb/run-20241007_185232-rrl2le5b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/rrl2le5b' target=\"_blank\">./New_video_llava_qlora</a></strong> to <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface' target=\"_blank\">https://wandb.ai/zeramarveenlyngkhoi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/rrl2le5b' target=\"_blank\">https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/rrl2le5b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  return torch.tensor(value)\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 21:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11.725000</td>\n",
       "      <td>11.926501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=11.620753943920135, metrics={'train_runtime': 1360.4581, 'train_samples_per_second': 0.098, 'train_steps_per_second': 0.012, 'total_flos': 2847163079983104.0, 'train_loss': 11.620753943920135, 'epoch': 0.9552238805970149})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a7cb563-9797-474f-9090-6d17e3e7d466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a64a4c21244d309fe50de9355cbc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/25.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904e100dd8de4ba5a9009e23cac61b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1728301736.dgx01.566391.0:   0%|          | 0.00/5.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8eceafd84f4665872250b1e89b990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1728301950.dgx01.569556.0:   0%|          | 0.00/9.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb675dd25f9423db64ae33a37fe950e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538e6eb9ed0b49b5b33d45c9ba0da2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zera09/New_video_llava_qlora/commit/385a8da9e6f2bb4769d91b5b264914c8dbb77bcd', commit_message='End of training', commit_description='', oid='385a8da9e6f2bb4769d91b5b264914c8dbb77bcd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13499796-10e9-4eb8-af5d-a6e249b26aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained('/home/sarmistha/Testing2/168_video_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d654c65e-fe82-40fd-9f9a-ada6916d82c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions...\n"
     ]
    }
   ],
   "source": [
    "processor = VideoLlavaProcessor.from_pretrained(\"LanguageBind/Video-LLaVA-7B-hf\")\n",
    "print(\"Generating predictions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "876e3db7-b5ab-478a-978a-0e28a3c3d163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Text</th>\n",
       "      <th>Video</th>\n",
       "      <th>Label</th>\n",
       "      <th>video_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>RV673Y47KY1QN</td>\n",
       "      <td>1.0 out of 5 stars\\n bad quality</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>bhai thodeor pese jod k acha wala lo y 15 day ...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/RV673Y47KY1QN.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RV98ZQO6NUFH7</td>\n",
       "      <td>1.0 out of 5 stars\\n I return the product...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Product is very very bad. And the packaging wh...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint that the receive key ...</td>\n",
       "      <td>/home/sarmistha/Research/videos/RV98ZQO6NUFH7.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>R566RA3J5VOMS</td>\n",
       "      <td>1.0 out of 5 stars\\n Old product delivered</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Dislike. I have got a old product instead of n...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R566RA3J5VOMS.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>R1LRZOJRXEJQYD</td>\n",
       "      <td>1.0 out of 5 stars\\n we got Broken Item</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Keyboard is broken please change this item</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint that the receive key ...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R1LRZOJRXEJQYD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>R3QQC9AI0YI2LM</td>\n",
       "      <td>1.0 out of 5 stars\\n Too small .</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>It's for kids... Medium size build hands can't...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R3QQC9AI0YI2LM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>R7TL6X5X306UX</td>\n",
       "      <td>2.0 out of 5 stars\\n Very small</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;2.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Very small</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R7TL6X5X306UX.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>R3FDNR85F3H0CW</td>\n",
       "      <td>1.0 out of 5 stars\\n Don't buy</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Am buy this for normal use not for gaming. Her...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R3FDNR85F3H0CW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>R2QI3KAZMYU0GQ</td>\n",
       "      <td>1.0 out of 5 stars\\n It is not working properl...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>It is not working properly.So I want to exchan...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R2QI3KAZMYU0GQ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>R1G3AIQX7WAAQX</td>\n",
       "      <td>2.0 out of 5 stars\\n Satisfied</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;2.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Mouse light not working</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R1G3AIQX7WAAQX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>RTUT4LNMQEN35</td>\n",
       "      <td>1.0 out of 5 stars\\n 8 key not coming back on ...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>8 key pressed in and not coming back on the ve...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint the keyboard is not f...</td>\n",
       "      <td>/home/sarmistha/Research/videos/RTUT4LNMQEN35.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>R3KTYW8LEA4QG3</td>\n",
       "      <td>1.0 out of 5 stars\\n Bed</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Quality is very bad with in four days it's was...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint that the received pro...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R3KTYW8LEA4QG3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>R1D5EPG1QJSN79</td>\n",
       "      <td>2.0 out of 5 stars\\n Very Small Size</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;2.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>The mouse size is very small according to my p...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R1D5EPG1QJSN79...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>R1R5RYBJWFHKWO</td>\n",
       "      <td>1.0 out of 5 stars\\n Worst product, stopped wo...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Worst product , stopped working in critical time.</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R1R5RYBJWFHKWO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>RHINNC8VA27YY</td>\n",
       "      <td>1.0 out of 5 stars\\n Defective product also af...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Waste of money.Bought this keyboard for the fi...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint the keyboard is broke...</td>\n",
       "      <td>/home/sarmistha/Research/videos/RHINNC8VA27YY.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>R2KZB7XI3RO0QA</td>\n",
       "      <td>1.0 out of 5 stars\\n Don't buy cheap product f...</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Broken one not marked as it is used very often.</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants to convey about the complaint o...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R2KZB7XI3RO0QA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>R3S6GAX26JTN61</td>\n",
       "      <td>1.0 out of 5 stars\\n Some keys are not working</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>Keys are not working in single click. I can't ...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint the keyboard is not f...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R3S6GAX26JTN61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>R2JZAC3U9GJ2QU</td>\n",
       "      <td>1.0 out of 5 stars\\n All the keys are broken</td>\n",
       "      <td>&lt;span class=\"a-icon-alt\"&gt;1.0 out of 5 stars&lt;/s...</td>\n",
       "      <td>I purchased Zebronics ZEB-KM2100 Multimedia US...</td>\n",
       "      <td>https://m.media-amazon.com/images/S/vse-vms-tr...</td>\n",
       "      <td>The user wants complaint that he received a da...</td>\n",
       "      <td>/home/sarmistha/Research/videos/R2JZAC3U9GJ2QU...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ID                                              Title  \\\n",
       "137   RV673Y47KY1QN                   1.0 out of 5 stars\\n bad quality   \n",
       "30    RV98ZQO6NUFH7       1.0 out of 5 stars\\n I return the product...   \n",
       "119   R566RA3J5VOMS         1.0 out of 5 stars\\n Old product delivered   \n",
       "29   R1LRZOJRXEJQYD            1.0 out of 5 stars\\n we got Broken Item   \n",
       "142  R3QQC9AI0YI2LM                   1.0 out of 5 stars\\n Too small .   \n",
       "161   R7TL6X5X306UX                    2.0 out of 5 stars\\n Very small   \n",
       "164  R3FDNR85F3H0CW                     1.0 out of 5 stars\\n Don't buy   \n",
       "51   R2QI3KAZMYU0GQ  1.0 out of 5 stars\\n It is not working properl...   \n",
       "105  R1G3AIQX7WAAQX                     2.0 out of 5 stars\\n Satisfied   \n",
       "60    RTUT4LNMQEN35  1.0 out of 5 stars\\n 8 key not coming back on ...   \n",
       "15   R3KTYW8LEA4QG3                           1.0 out of 5 stars\\n Bed   \n",
       "156  R1D5EPG1QJSN79               2.0 out of 5 stars\\n Very Small Size   \n",
       "133  R1R5RYBJWFHKWO  1.0 out of 5 stars\\n Worst product, stopped wo...   \n",
       "45    RHINNC8VA27YY  1.0 out of 5 stars\\n Defective product also af...   \n",
       "68   R2KZB7XI3RO0QA  1.0 out of 5 stars\\n Don't buy cheap product f...   \n",
       "85   R3S6GAX26JTN61     1.0 out of 5 stars\\n Some keys are not working   \n",
       "24   R2JZAC3U9GJ2QU       1.0 out of 5 stars\\n All the keys are broken   \n",
       "\n",
       "                                                Rating  \\\n",
       "137  <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "30   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "119  <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "29   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "142  <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "161  <span class=\"a-icon-alt\">2.0 out of 5 stars</s...   \n",
       "164  <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "51   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "105  <span class=\"a-icon-alt\">2.0 out of 5 stars</s...   \n",
       "60   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "15   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "156  <span class=\"a-icon-alt\">2.0 out of 5 stars</s...   \n",
       "133  <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "45   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "68   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "85   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "24   <span class=\"a-icon-alt\">1.0 out of 5 stars</s...   \n",
       "\n",
       "                                                  Text  \\\n",
       "137  bhai thodeor pese jod k acha wala lo y 15 day ...   \n",
       "30   Product is very very bad. And the packaging wh...   \n",
       "119  Dislike. I have got a old product instead of n...   \n",
       "29          Keyboard is broken please change this item   \n",
       "142  It's for kids... Medium size build hands can't...   \n",
       "161                                         Very small   \n",
       "164  Am buy this for normal use not for gaming. Her...   \n",
       "51   It is not working properly.So I want to exchan...   \n",
       "105                            Mouse light not working   \n",
       "60   8 key pressed in and not coming back on the ve...   \n",
       "15   Quality is very bad with in four days it's was...   \n",
       "156  The mouse size is very small according to my p...   \n",
       "133  Worst product , stopped working in critical time.   \n",
       "45   Waste of money.Bought this keyboard for the fi...   \n",
       "68     Broken one not marked as it is used very often.   \n",
       "85   Keys are not working in single click. I can't ...   \n",
       "24   I purchased Zebronics ZEB-KM2100 Multimedia US...   \n",
       "\n",
       "                                                 Video  \\\n",
       "137  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "30   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "119  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "29   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "142  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "161  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "164  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "51   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "105  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "60   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "15   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "156  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "133  https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "45   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "68   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "85   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "24   https://m.media-amazon.com/images/S/vse-vms-tr...   \n",
       "\n",
       "                                                 Label  \\\n",
       "137  The user wants to convey about the complaint o...   \n",
       "30   The user wants complaint that the receive key ...   \n",
       "119  The user wants to convey about the complaint o...   \n",
       "29   The user wants complaint that the receive key ...   \n",
       "142  The user wants to convey about the complaint o...   \n",
       "161  The user wants to convey about the complaint o...   \n",
       "164  The user wants to convey about the complaint o...   \n",
       "51   The user wants to convey about the complaint o...   \n",
       "105  The user wants to convey about the complaint o...   \n",
       "60   The user wants complaint the keyboard is not f...   \n",
       "15   The user wants complaint that the received pro...   \n",
       "156  The user wants to convey about the complaint o...   \n",
       "133  The user wants to convey about the complaint o...   \n",
       "45   The user wants complaint the keyboard is broke...   \n",
       "68   The user wants to convey about the complaint o...   \n",
       "85   The user wants complaint the keyboard is not f...   \n",
       "24   The user wants complaint that he received a da...   \n",
       "\n",
       "                                            video_path  \n",
       "137  /home/sarmistha/Research/videos/RV673Y47KY1QN.mp4  \n",
       "30   /home/sarmistha/Research/videos/RV98ZQO6NUFH7.mp4  \n",
       "119  /home/sarmistha/Research/videos/R566RA3J5VOMS.mp4  \n",
       "29   /home/sarmistha/Research/videos/R1LRZOJRXEJQYD...  \n",
       "142  /home/sarmistha/Research/videos/R3QQC9AI0YI2LM...  \n",
       "161  /home/sarmistha/Research/videos/R7TL6X5X306UX.mp4  \n",
       "164  /home/sarmistha/Research/videos/R3FDNR85F3H0CW...  \n",
       "51   /home/sarmistha/Research/videos/R2QI3KAZMYU0GQ...  \n",
       "105  /home/sarmistha/Research/videos/R1G3AIQX7WAAQX...  \n",
       "60   /home/sarmistha/Research/videos/RTUT4LNMQEN35.mp4  \n",
       "15   /home/sarmistha/Research/videos/R3KTYW8LEA4QG3...  \n",
       "156  /home/sarmistha/Research/videos/R1D5EPG1QJSN79...  \n",
       "133  /home/sarmistha/Research/videos/R1R5RYBJWFHKWO...  \n",
       "45   /home/sarmistha/Research/videos/RHINNC8VA27YY.mp4  \n",
       "68   /home/sarmistha/Research/videos/R2KZB7XI3RO0QA...  \n",
       "85   /home/sarmistha/Research/videos/R3S6GAX26JTN61...  \n",
       "24   /home/sarmistha/Research/videos/R2JZAC3U9GJ2QU...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8999715-2fb9-4bc3-a29c-3df7602da767",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('/home/sarmistha/Testing2/168_video_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec12ad05-eaab-42d2-a61d-b79b156869cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RV673Y47KY1QN [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the mouse's poor performance.\\n\\nThe user is holding a black computer mouse and pointing to it, indicating that they are unhappy with its performance. They may be complaining about the mouse's responsiveness, accuracy, or overall functionality. This could be due to issues such as lagging, unresponsiveness, or inaccurate tracking, which can affect the user's experience while working on a computer.\"]\n",
      "RV98ZQO6NUFH7 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the keyboard, specifically the keyboard's keys. (or buttons) being broken. They are holding the keyboard in their hand and pointing to the broken keys, which suggests that the keyboard is not functioning properly. This complaint highlights the user's dissatisfaction with the keyboard's performance and may indicate a need for repair or replacement.\"]\n",
      "R566RA3J5VOMS [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the charging cable, which is not working properly. (or is not working at all). They are holding the charging cable in their hand and pointing to it, indicating that it is not functioning as expected. This complaint highlights the user's frustration with the charging cable and may suggest that they need to find a replacement or a solution to the problem.\"]\n",
      "R1LRZOJRXEJQYD [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the keyboard's design, specifically the keys' arrangement.c\"]\n",
      "R3QQC9AI0YI2LM ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the HP mouse, which is placed on the table next to the other mouse.c']\n",
      "R7TL6X5X306UX [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the mouse's poor performance. (or lack of performance) and is expressing dissatisfaction with its functionality.\"]\n",
      "R3FDNR85F3H0CW [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the mouse's performance. (or lack thereof) by holding it up and pointing to it. This could be due to issues such as poor responsiveness, inaccurate tracking, or other problems that the user finds frustrating.\"]\n",
      "R2QI3KAZMYU0GQ ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard\\'s poor quality. (or \"bad\" quality) and its inability to work properly. They are expressing dissatisfaction with the keyboard\\'s performance and are likely seeking a replacement or a better keyboard.']\n",
      "R1G3AIQX7WAAQX [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's blue color, which is not their preferred choice. (The user prefers a white keyboard).\"]\n",
      "RTUT4LNMQEN35 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the keyboard's layout and design.\"]\n",
      "R3KTYW8LEA4QG3 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the bag being too small to hold all of their belongings. (specifically, a wallet and a passport). They are expressing dissatisfaction with the bag's capacity and are looking for a larger bag to accommodate their items.\"]\n",
      "R1D5EPG1QJSN79 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the keyboard's design, specifically the lack of a scroll wheel.c\"]\n",
      "R1R5RYBJWFHKWO ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard of the laptop.c']\n",
      "RHINNC8VA27YY [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's keys being stuck together. (sticky keys). This issue can cause inconvenience and frustration for the user while typing, as they may have to press multiple keys at once or struggle to type accurately.\"]\n",
      "R2KZB7XI3RO0QA [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's keys being stuck together. (sticking keys).\"]\n",
      "R3S6GAX26JTN61 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's blue color, which they find unattractive.c\"]\n",
      "R2JZAC3U9GJ2QU [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing dissatisfaction with the keyboard's design, specifically the missing key. (the X key) and the missing keycaps. They are pointing out the flaws in the keyboard's design, which may affect the user's experience while typing.\"]\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "num_frames=8\n",
    "max_length=512\n",
    "video_dir = '/home/sarmistha/Research/videos'\n",
    "\n",
    "\n",
    "def get_inputs(row):\n",
    "        video_file = row['ID'] + '.mp4'\n",
    "        label = row['Text']\n",
    "\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        container = av.open(video_path)\n",
    "        total_frames = container.streams.video[0].frames\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        video = read_video_pyav(container, indices)\n",
    "        \n",
    "        prompt = \"USER: <video>What complaint is conveyed by the user in the video? ASSISTANT:\"\n",
    "        \n",
    "        inputs = processor(text=prompt, videos=video, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length, truncation=True)\n",
    "        #labels = processor(text=label, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length, truncation=True)['input_ids'].squeeze(0)\n",
    "        \n",
    "        # inputs['labels'] = labels\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/sarmistha/Testing2/168_video_test.csv')\n",
    "\n",
    "prediction_list=[]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    inputs = get_inputs(row)\n",
    "    out = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    prediction = processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(row['ID'], prediction)\n",
    "    prediction_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686787c1-9a2e-4ff0-9b40-90ef3861cdc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1f5b5-d9b5-4c64-8c25-e68a9c43df2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d08d9-b9bd-41bf-a39c-2f7ca0556b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520187c-1f86-4520-9c34-d156e273fdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d90eff-3ce5-4e3d-8e27-d722760d257a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5528a865-8f74-48a2-98a0-7703cbbd2395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afb7fc5c-b02b-459e-b07f-d9e4230357d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/sarmistha/miniconda3/envs/vinayak/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RV673Y47KY1QN ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the mouse not working properly. (or not working at all) and is trying to fix the problem by pressing the mouse button.']\n",
      "RV98ZQO6NUFH7 ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard being broken. (or damaged) and is holding it up to show the damage.']\n",
      "R566RA3J5VOMS ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the black cord of the device. (USB) that is being plugged into the device.']\n",
      "R1LRZOJRXEJQYD [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing their dissatisfaction with the keyboard.\\n\\nThe user is holding the keyboard in their hand and pointing to it, indicating that they are unhappy with the keyboard. The reason for their dissatisfaction is not clear from the video, but it could be related to the keyboard's design, functionality, or overall quality. The user's dissatisfaction highlights the importance of user feedback and the need for companies to address any issues or concerns raised\"]\n",
      "R3QQC9AI0YI2LM ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is demonstrating how to use a computer mouse.\\n\\nThe video shows a person holding a computer mouse and pointing to it. The person then proceeds to demonstrate how to use the mouse. The video is likely intended to provide instructions or guidance on how to use a computer mouse.']\n",
      "R7TL6X5X306UX [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the mouse's poor performance. (or lack of performance) and is asking for a new one.\"]\n",
      "R3FDNR85F3H0CW [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is expressing their dissatisfaction with the mouse's performance. (or lack thereof) by holding it in their hand and making a negative gesture.\"]\n",
      "R2QI3KAZMYU0GQ [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's poor quality. (or lack of quality) and its inability to function properly.\"]\n",
      "R1G3AIQX7WAAQX [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's blue color. (or blue light) and its size.\"]\n",
      "RTUT4LNMQEN35 ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is demonstrating how to use a keyboard.']\n",
      "R3KTYW8LEA4QG3 ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is conveying that the bag is very heavy..']\n",
      "R1D5EPG1QJSN79 [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's keys being stuck together. (sticking keys).\"]\n",
      "R1R5RYBJWFHKWO [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard of the laptop.\\n\\nThe user is pointing to the keyboard of the laptop, indicating that they are unhappy with it. The reason for their dissatisfaction is not clear from the video, but it could be related to the keyboard's design, functionality, or overall quality. The user's complaint highlights the importance of user satisfaction and the need for manufacturers to address any issues or concerns that may arise with their\"]\n",
      "RHINNC8VA27YY [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's keys being stuck together. (sticky keys).\"]\n",
      "R2KZB7XI3RO0QA [\"USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard's keys being too close together. (or too far apart) and the lack of a numeric keypad.\"]\n",
      "R3S6GAX26JTN61 ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard being blue.c']\n",
      "R2JZAC3U9GJ2QU ['USER: What complaint is conveyed by the user in the video? ASSISTANT: The user in the video is complaining about the keyboard. (The keyboard is not working properly).']\n"
     ]
    }
   ],
   "source": [
    "import av\n",
    "import numpy as np\n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (av.container.input.InputContainer): PyAV container.\n",
    "        indices (List[int]): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "num_frames=8\n",
    "max_length=512\n",
    "video_dir = '/home/sarmistha/Research/videos'\n",
    "\n",
    "\n",
    "def get_inputs(row):\n",
    "        video_file = row['ID'] + '.mp4'\n",
    "        label = row['Text']\n",
    "\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        container = av.open(video_path)\n",
    "        total_frames = container.streams.video[0].frames\n",
    "        indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "        video = read_video_pyav(container, indices)\n",
    "        \n",
    "        prompt = \"USER: <video>What complaint is conveyed by the user in the video? ASSISTANT:\"\n",
    "        \n",
    "        inputs = processor(text=prompt, videos=video, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length, truncation=True)\n",
    "        #labels = processor(text=label, return_tensors=\"pt\", padding=\"max_length\", max_length=max_length, truncation=True)['input_ids'].squeeze(0)\n",
    "        \n",
    "        # inputs['labels'] = labels\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/sarmistha/Testing2/168_video_test.csv')\n",
    "\n",
    "prediction_list=[]\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    inputs = get_inputs(row)\n",
    "    out = model.generate(**inputs, max_new_tokens=100)\n",
    "    \n",
    "    prediction = processor.batch_decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "    print(row['ID'], prediction)\n",
    "    prediction_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8823469-f0b9-47f3-b394-6ab2fd615f6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'videollama2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvideollama2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_init, mm_infer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvideollama2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_torch_init\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'videollama2'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "from videollama2 import model_init, mm_infer\n",
    "from videollama2.utils import disable_torch_init\n",
    "\n",
    "\n",
    "def inference():\n",
    "    disable_torch_init()\n",
    "\n",
    "    # Video Inference\n",
    "    modal = 'video'\n",
    "    modal_path = 'Research/videos/R10J0N53PGXZS4.mp4' \n",
    "    instruct = \"USER: <video>What complaint is conveyed by the user in the video? ASSISTANT:\"\n",
    "   \n",
    "\n",
    "    model_path = 'DAMO-NLP-SG/VideoLLaMA2-7B-Base'\n",
    "    model, processor, tokenizer = model_init(model_path)\n",
    "    output = mm_infer(processor[modal](modal_path), instruct, model=model, tokenizer=tokenizer, do_sample=False, modal=modal)\n",
    "\n",
    "    print(output)\n",
    "inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416dad58-6d21-44fc-929a-f1f2fce1458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/home/sarmistha/miniconda3/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mPackage                  Version\n",
      "------------------------ ------------------\n",
      "absl-py                  2.1.0\n",
      "accelerate               1.0.0\n",
      "aiohappyeyeballs         2.4.3\n",
      "aiohttp                  3.10.9\n",
      "aiosignal                1.3.1\n",
      "anaconda-anon-usage      0.4.4\n",
      "archspec                 0.2.3\n",
      "asttokens                2.4.1\n",
      "attrs                    24.2.0\n",
      "av                       13.0.0\n",
      "blessed                  1.20.0\n",
      "boltons                  23.0.0\n",
      "Brotli                   1.0.9\n",
      "certifi                  2024.7.4\n",
      "cffi                     1.16.0\n",
      "charset-normalizer       3.3.2\n",
      "click                    8.1.7\n",
      "cloudpickle              3.1.0\n",
      "comm                     0.2.2\n",
      "conda                    24.7.1\n",
      "conda-content-trust      0.2.0\n",
      "conda-libmamba-solver    24.7.0\n",
      "conda-package-handling   2.3.0\n",
      "conda_package_streaming  0.10.0\n",
      "contourpy                1.3.0\n",
      "cryptography             42.0.5\n",
      "cycler                   0.12.1\n",
      "datasets                 3.0.1\n",
      "debugpy                  1.8.5\n",
      "decorator                5.1.1\n",
      "decord                   0.6.0\n",
      "dill                     0.3.8\n",
      "distro                   1.9.0\n",
      "docstring_parser         0.16\n",
      "einops                   0.8.0\n",
      "evaluate                 0.4.3\n",
      "executing                2.1.0\n",
      "Farama-Notifications     0.0.4\n",
      "filelock                 3.16.1\n",
      "fonttools                4.54.1\n",
      "frozendict               2.4.2\n",
      "frozenlist               1.4.1\n",
      "fsspec                   2024.6.1\n",
      "fvcore                   0.1.5.post20221221\n",
      "gpustat                  1.1.1\n",
      "grpcio                   1.66.2\n",
      "gymnasium                1.0.0\n",
      "huggingface-hub          0.25.0\n",
      "idna                     3.7\n",
      "iopath                   0.1.10\n",
      "ipykernel                6.29.5\n",
      "ipython                  8.27.0\n",
      "jedi                     0.19.1\n",
      "Jinja2                   3.1.4\n",
      "joblib                   1.4.2\n",
      "jsonpatch                1.33\n",
      "jsonpointer              2.1\n",
      "jupyter_client           8.6.2\n",
      "jupyter_core             5.7.2\n",
      "kiwisolver               1.4.7\n",
      "libmambapy               1.5.8\n",
      "Markdown                 3.7\n",
      "markdown-it-py           3.0.0\n",
      "MarkupSafe               2.1.5\n",
      "matplotlib               3.9.2\n",
      "matplotlib-inline        0.1.7\n",
      "mdurl                    0.1.2\n",
      "menuinst                 2.1.2\n",
      "mpmath                   1.3.0\n",
      "multidict                6.1.0\n",
      "multiprocess             0.70.16\n",
      "nest-asyncio             1.6.0\n",
      "networkx                 3.3\n",
      "nltk                     3.9.1\n",
      "numpy                    2.1.1\n",
      "nvidia-cublas-cu12       12.1.3.1\n",
      "nvidia-cuda-cupti-cu12   12.1.105\n",
      "nvidia-cuda-nvrtc-cu12   12.1.105\n",
      "nvidia-cuda-runtime-cu12 12.1.105\n",
      "nvidia-cudnn-cu12        8.9.2.26\n",
      "nvidia-cufft-cu12        11.0.2.54\n",
      "nvidia-curand-cu12       10.3.2.106\n",
      "nvidia-cusolver-cu12     11.4.5.107\n",
      "nvidia-cusparse-cu12     12.1.0.106\n",
      "nvidia-ml-py             12.560.30\n",
      "nvidia-nccl-cu12         2.19.3\n",
      "nvidia-nvjitlink-cu12    12.6.68\n",
      "nvidia-nvtx-cu12         12.1.105\n",
      "opencv-python            4.10.0.84\n",
      "packaging                24.1\n",
      "pandas                   2.2.3\n",
      "parameterized            0.9.0\n",
      "parso                    0.8.4\n",
      "peft                     0.6.0\n",
      "pexpect                  4.9.0\n",
      "pillow                   10.4.0\n",
      "pip                      24.2\n",
      "platformdirs             3.10.0\n",
      "pluggy                   1.0.0\n",
      "portalocker              2.10.1\n",
      "prompt_toolkit           3.0.47\n",
      "protobuf                 5.28.2\n",
      "psutil                   6.0.0\n",
      "ptyprocess               0.7.0\n",
      "pure_eval                0.2.3\n",
      "pyarrow                  17.0.0\n",
      "pycosat                  0.6.6\n",
      "pycparser                2.21\n",
      "Pygments                 2.18.0\n",
      "pyparsing                3.2.0\n",
      "PySocks                  1.7.1\n",
      "python-dateutil          2.9.0.post0\n",
      "pytorchvideo             0.1.5\n",
      "pytz                     2024.2\n",
      "PyYAML                   6.0.2\n",
      "pyzmq                    26.2.0\n",
      "regex                    2024.9.11\n",
      "requests                 2.32.3\n",
      "rich                     13.9.2\n",
      "rouge_score              0.1.2\n",
      "ruamel.yaml              0.17.21\n",
      "safetensors              0.4.5\n",
      "sentencepiece            0.2.0\n",
      "setuptools               72.1.0\n",
      "shtab                    1.7.1\n",
      "six                      1.16.0\n",
      "stack-data               0.6.3\n",
      "sympy                    1.13.2\n",
      "tabulate                 0.9.0\n",
      "tensorboard              2.18.0\n",
      "tensorboard-data-server  0.7.2\n",
      "termcolor                2.4.0\n",
      "tokenizers               0.19.1\n",
      "torch                    2.2.2\n",
      "tornado                  6.4.1\n",
      "tqdm                     4.66.4\n",
      "traitlets                5.14.3\n",
      "transformers             4.44.2\n",
      "triton                   3.0.0\n",
      "trl                      0.11.2\n",
      "truststore               0.8.0\n",
      "typing_extensions        4.12.2\n",
      "tyro                     0.8.11\n",
      "tzdata                   2024.2\n",
      "urllib3                  2.2.2\n",
      "wcwidth                  0.2.13\n",
      "Werkzeug                 3.0.4\n",
      "wheel                    0.43.0\n",
      "xxhash                   3.5.0\n",
      "yacs                     0.1.8\n",
      "yarl                     1.13.1\n",
      "zstandard                0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92ee488-8e65-4056-b09a-e15f97d2e9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeMix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
